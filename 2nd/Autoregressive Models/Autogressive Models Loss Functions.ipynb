{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Categorical Distribution Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gaussian Mixture Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Mixture Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logstic Distribution의 PDF(Probability Distribution Function)\n",
    "\\begin{aligned}\n",
    "f(x ; \\mu, s) &=\\frac{1}{4 s} \\operatorname{sech}^{2}\\left(\\frac{x-\\mu}{2 s}\\right)\n",
    "\\end{aligned}\n",
    "\n",
    "<img src=\"pics/logistic_pdf.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logstic Distribution의 CDF(Probability Distribution Function)\n",
    "\\begin{aligned}\n",
    "F(x ; \\mu, s)=\\frac{1}{1+e^{-(x-\\mu) / s}}\n",
    "\\end{aligned}\n",
    "\n",
    "<img src=\"pics/logistic_cdf.png\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic distribution은 logistic function을 CDF로 갖는 distribution입니다.\n",
    "2. Logistic function은 lower-bound가 0이고 upper-bound가 1인 단조 증가 함수이므로 CDF로 사용가능합니다.\n",
    "3. Parameters로 mean을 나타내는 $\\mu$와 scale을 나타내는 $s$가 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. OpenAI PixelCNN++ source : Discretized Logistic Mixture가 처음 사용된 모델, 구현\n",
    "### https://github.com/openai/pixel-cnn/blob/master/pixel_cnn_pp/nn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretized_mix_logistic_loss(x,l,sum_all=True):\n",
    "    \"\"\" log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval \"\"\"\n",
    "    xs = int_shape(x) # true image (i.e. labels) to regress to, e.g. (B,32,32,3)\n",
    "    ls = int_shape(l) # predicted distribution, e.g. (B,32,32,100)\n",
    "    nr_mix = int(ls[-1] / 10) # here and below: unpacking the params of the mixture of logistics\n",
    "    logit_probs = l[:,:,:,:nr_mix]\n",
    "    l = tf.reshape(l[:,:,:,nr_mix:], xs + [nr_mix*3])\n",
    "    means = l[:,:,:,:,:nr_mix]\n",
    "    log_scales = tf.maximum(l[:,:,:,:,nr_mix:2*nr_mix], -7.)\n",
    "    coeffs = tf.nn.tanh(l[:,:,:,:,2*nr_mix:3*nr_mix])\n",
    "    x = tf.reshape(x, xs + [1]) + tf.zeros(xs + [nr_mix]) # here and below: getting the means and adjusting them based on preceding sub-pixels\n",
    "    m2 = tf.reshape(means[:,:,:,1,:] + coeffs[:, :, :, 0, :] * x[:, :, :, 0, :], [xs[0],xs[1],xs[2],1,nr_mix])\n",
    "    m3 = tf.reshape(means[:, :, :, 2, :] + coeffs[:, :, :, 1, :] * x[:, :, :, 0, :] + coeffs[:, :, :, 2, :] * x[:, :, :, 1, :], [xs[0],xs[1],xs[2],1,nr_mix])\n",
    "    means = tf.concat([tf.reshape(means[:,:,:,0,:], [xs[0],xs[1],xs[2],1,nr_mix]), m2, m3],3)\n",
    "    centered_x = x - means\n",
    "    inv_stdv = tf.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_x + 1./255.)\n",
    "    cdf_plus = tf.nn.sigmoid(plus_in)\n",
    "    min_in = inv_stdv * (centered_x - 1./255.)\n",
    "    cdf_min = tf.nn.sigmoid(min_in)\n",
    "    log_cdf_plus = plus_in - tf.nn.softplus(plus_in) # log probability for edge case of 0 (before scaling)\n",
    "    log_one_minus_cdf_min = -tf.nn.softplus(min_in) # log probability for edge case of 255 (before scaling)\n",
    "    cdf_delta = cdf_plus - cdf_min # probability for all other cases\n",
    "    mid_in = inv_stdv * centered_x\n",
    "    log_pdf_mid = mid_in - log_scales - 2.*tf.nn.softplus(mid_in) # log probability in the center of the bin, to be used in extreme cases (not actually used in our code)\n",
    "\n",
    "    # now select the right output: left edge case, right edge case, normal case, extremely low prob case (doesn't actually happen for us)\n",
    "\n",
    "    # this is what we are really doing, but using the robust version below for extreme cases in other applications and to avoid NaN issue with tf.select()\n",
    "    # log_probs = tf.select(x < -0.999, log_cdf_plus, tf.select(x > 0.999, log_one_minus_cdf_min, tf.log(cdf_delta)))\n",
    "\n",
    "    # robust version, that still works if probabilities are below 1e-5 (which never happens in our code)\n",
    "    # tensorflow backpropagates through tf.select() by multiplying with zero instead of selecting: this requires use to use some ugly tricks to avoid potential NaNs\n",
    "    # the 1e-12 in tf.maximum(cdf_delta, 1e-12) is never actually used as output, it's purely there to get around the tf.select() gradient issue\n",
    "    # if the probability on a sub-pixel is below 1e-5, we use an approximation based on the assumption that the log-density is constant in the bin of the observed sub-pixel value\n",
    "    log_probs = tf.where(x < -0.999, log_cdf_plus, tf.where(x > 0.999, log_one_minus_cdf_min, tf.where(cdf_delta > 1e-5, tf.log(tf.maximum(cdf_delta, 1e-12)), log_pdf_mid - np.log(127.5))))\n",
    "\n",
    "    log_probs = tf.reduce_sum(log_probs,3) + log_prob_from_logits(logit_probs)\n",
    "    if sum_all:\n",
    "        return -tf.reduce_sum(log_sum_exp(log_probs))\n",
    "    else:\n",
    "        return -tf.reduce_sum(log_sum_exp(log_probs),[1,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_discretized_mix_logistic(l,nr_mix):\n",
    "    ls = int_shape(l)\n",
    "    xs = ls[:-1] + [3]\n",
    "    # unpack parameters\n",
    "    logit_probs = l[:, :, :, :nr_mix]\n",
    "    l = tf.reshape(l[:, :, :, nr_mix:], xs + [nr_mix*3])\n",
    "    # sample mixture indicator from softmax\n",
    "    sel = tf.one_hot(tf.argmax(logit_probs - tf.log(-tf.log(tf.random_uniform(logit_probs.get_shape(), minval=1e-5, maxval=1. - 1e-5))), 3), depth=nr_mix, dtype=tf.float32)\n",
    "    sel = tf.reshape(sel, xs[:-1] + [1,nr_mix])\n",
    "    # select logistic parameters\n",
    "    means = tf.reduce_sum(l[:,:,:,:,:nr_mix]*sel,4)\n",
    "    log_scales = tf.maximum(tf.reduce_sum(l[:,:,:,:,nr_mix:2*nr_mix]*sel,4), -7.)\n",
    "    coeffs = tf.reduce_sum(tf.nn.tanh(l[:,:,:,:,2*nr_mix:3*nr_mix])*sel,4)\n",
    "    # sample from logistic & clip to interval\n",
    "    # we don't actually round to the nearest 8bit value when sampling\n",
    "    u = tf.random_uniform(means.get_shape(), minval=1e-5, maxval=1. - 1e-5)\n",
    "    x = means + tf.exp(log_scales)*(tf.log(u) - tf.log(1. - u))\n",
    "    x0 = tf.minimum(tf.maximum(x[:,:,:,0], -1.), 1.)\n",
    "    x1 = tf.minimum(tf.maximum(x[:,:,:,1] + coeffs[:,:,:,0]*x0, -1.), 1.)\n",
    "    x2 = tf.minimum(tf.maximum(x[:,:,:,2] + coeffs[:,:,:,1]*x0 + coeffs[:,:,:,2]*x1, -1.), 1.)\n",
    "    return tf.concat([tf.reshape(x0,xs[:-1]+[1]), tf.reshape(x1,xs[:-1]+[1]), tf.reshape(x2,xs[:-1]+[1])],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. r9y9 Wavenet 구현, Pytorch로 구현되어 있으며 코드가 좀 더 깔끔하게 정리되어 있다.\n",
    "### https://github.com/openai/pixel-cnn/blob/master/pixel_cnn_pp/nn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters \n",
    "K: cluster 갯수, line17 nr_mix(number of mixture)로 받아옴 <br>\n",
    "$\\pi_k$: mixing coefficients, line23 logit_probs로 softmax적용되기 이전 값으로 존재 <br>\n",
    "$\\mu_k$: means, line24 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretized_mix_logistic_loss(y_hat, y, num_classes=256,\n",
    "                                  log_scale_min=-7.0, reduce=True):\n",
    "    \"\"\"Discretized mixture of logistic distributions loss\n",
    "    Note that it is assumed that input is scaled to [-1, 1].\n",
    "    Args:\n",
    "        y_hat (Tensor): Predicted output (B x C x T)\n",
    "        y (Tensor): Target (B x T x 1).\n",
    "        num_classes (int): Number of classes\n",
    "        log_scale_min (float): Log scale minimum value\n",
    "        reduce (bool): If True, the losses are averaged or summed for each\n",
    "          minibatch.\n",
    "    Returns\n",
    "        Tensor: loss\n",
    "    \"\"\"\n",
    "    assert y_hat.dim() == 3\n",
    "    assert y_hat.size(1) % 3 == 0\n",
    "    nr_mix = y_hat.size(1) // 3\n",
    "\n",
    "    # (B x T x C)\n",
    "    y_hat = y_hat.transpose(1, 2)\n",
    "\n",
    "    # unpack parameters. (B, T, num_mixtures) x 3\n",
    "    logit_probs = y_hat[:, :, :nr_mix]\n",
    "    means = y_hat[:, :, nr_mix:2 * nr_mix]\n",
    "    log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix:3 * nr_mix], min=log_scale_min)\n",
    "\n",
    "    # B x T x 1 -> B x T x num_mixtures\n",
    "    y = y.expand_as(means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    centered_y = y - means\n",
    "    inv_stdv = torch.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_y + 1. / (num_classes - 1))\n",
    "    cdf_plus = torch.sigmoid(plus_in)\n",
    "    min_in = inv_stdv * (centered_y - 1. / (num_classes - 1))\n",
    "    cdf_min = torch.sigmoid(min_in)\n",
    "\n",
    "    # log probability for edge case of 0 (before scaling)\n",
    "    # equivalent: torch.log(torch.sigmoid(plus_in))\n",
    "    log_cdf_plus = plus_in - F.softplus(plus_in)\n",
    "\n",
    "    # log probability for edge case of 255 (before scaling)\n",
    "    # equivalent: (1 - torch.sigmoid(min_in)).log()\n",
    "    log_one_minus_cdf_min = -F.softplus(min_in)\n",
    "\n",
    "    # probability for all other cases\n",
    "    cdf_delta = cdf_plus - cdf_min\n",
    "\n",
    "    mid_in = inv_stdv * centered_y\n",
    "    # log probability in the center of the bin, to be used in extreme cases\n",
    "    # (not actually used in our code)\n",
    "    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)\n",
    "\n",
    "    # tf equivalent\n",
    "    \"\"\"\n",
    "    log_probs = tf.where(x < -0.999, log_cdf_plus,\n",
    "                         tf.where(x > 0.999, log_one_minus_cdf_min,\n",
    "                                  tf.where(cdf_delta > 1e-5,\n",
    "                                           tf.log(tf.maximum(cdf_delta, 1e-12)),\n",
    "                                           log_pdf_mid - np.log(127.5))))\n",
    "    \"\"\"\n",
    "    # TODO: cdf_delta <= 1e-5 actually can happen. How can we choose the value\n",
    "    # for num_classes=65536 case? 1e-7? not sure..\n",
    "    inner_inner_cond = (cdf_delta > 1e-5).float()\n",
    "\n",
    "    inner_inner_out = inner_inner_cond * \\\n",
    "        torch.log(torch.clamp(cdf_delta, min=1e-12)) + \\\n",
    "        (1. - inner_inner_cond) * (log_pdf_mid - np.log((num_classes - 1) / 2))\n",
    "    inner_cond = (y > 0.999).float()\n",
    "    inner_out = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n",
    "    cond = (y < -0.999).float()\n",
    "    log_probs = cond * log_cdf_plus + (1. - cond) * inner_out\n",
    "\n",
    "    log_probs = log_probs + F.log_softmax(logit_probs, -1)\n",
    "\n",
    "    if reduce:\n",
    "        return -torch.sum(log_sum_exp(log_probs))\n",
    "    else:\n",
    "        return -log_sum_exp(log_probs).unsqueeze(-1)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
